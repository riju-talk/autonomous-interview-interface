# Architecture Overview

Technical architecture and design decisions for the AI Excel Interview platform.

## üèóÔ∏è System Architecture

```mermaid
graph TB
    subgraph "Frontend Layer"
        UI[React UI Components]
        STATE[State Management]
        ROUTER[React Router]
    end
    
    subgraph "API Gateway"
        FASTAPI[FastAPI Server]
        CORS[CORS Middleware]
        AUTH[Authentication]
    end
    
    subgraph "Business Logic"
        INTERVIEW[Interview Service]
        EVAL[Evaluation Service]
        QUESTIONS[Question Management]
    end
    
    subgraph "AI/ML Layer"
        LLM[LLM Service]
        GROQ[Groq API]
        OPENAI[OpenAI API]
        CHROMA[ChromaDB]
        EMBED[Embeddings]
    end
    
    subgraph "Data Layer"
        SQLITE[SQLite Database]
        REDIS[Redis Cache]
        FILES[File Storage]
    end
    
    UI --> FASTAPI
    STATE --> FASTAPI
    ROUTER --> FASTAPI
    
    FASTAPI --> INTERVIEW
    FASTAPI --> EVAL
    FASTAPI --> QUESTIONS
    
    INTERVIEW --> LLM
    EVAL --> LLM
    QUESTIONS --> CHROMA
    
    LLM --> GROQ
    LLM --> OPENAI
    
    CHROMA --> EMBED
    
    INTERVIEW --> SQLITE
    EVAL --> SQLITE
    QUESTIONS --> SQLITE
    
    INTERVIEW --> REDIS
    EVAL --> REDIS
    
    FILES --> SQLITE
```

## üéØ Design Principles

### 1. **Modular Architecture**
- **Services**: Loosely coupled, single responsibility
- **Layers**: Clear separation between UI, API, business logic, and data
- **Dependencies**: Minimal coupling, dependency injection ready

### 2. **AI-First Design**
- **LLM Integration**: Multiple providers with fallbacks
- **Vector Search**: ChromaDB for semantic question similarity
- **Prompt Engineering**: Template-based, version controlled
- **Evaluation Pipeline**: Multi-criteria scoring with confidence metrics

### 3. **Developer Experience**
- **Type Safety**: Full TypeScript frontend, Pydantic backend
- **API Documentation**: Auto-generated with FastAPI
- **Hot Reload**: Both frontend and backend development servers
- **Testing Ready**: Structured for unit and integration tests

### 4. **Production Ready**
- **Error Handling**: Graceful degradation and fallbacks
- **Logging**: Structured logging with different levels
- **Configuration**: Environment-based settings
- **Monitoring**: Health checks and metrics ready

## üìÅ Directory Structure

```
‚îú‚îÄ‚îÄ apps/
‚îÇ   ‚îú‚îÄ‚îÄ web/                    # React Frontend
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ components/     # React components
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ui/         # shadcn/ui components
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ LandingPage.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Instructions.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ InterviewWindow.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Completion.tsx
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ hooks/          # Custom React hooks
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ lib/            # Utilities and API client
‚îÇ   ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pages/          # Route components
‚îÇ   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ App.tsx         # Main application
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ public/             # Static assets
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ package.json        # Dependencies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ vite.config.ts      # Vite configuration
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ .env                # Environment variables
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ api/                    # FastAPI Backend
‚îÇ       ‚îú‚îÄ‚îÄ app/
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ core/           # Core infrastructure
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py   # Configuration management
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ database.py # Database setup
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ redis.py    # Redis client
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ models/         # Database models
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interview.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ schemas/        # Pydantic schemas
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ base.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ user.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ interview.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ services/       # Business logic
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm_service.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ chroma_service.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ auth_service.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ routers/        # API endpoints
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ interview.py
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ sessions.py
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ prompts/        # LLM prompt templates
‚îÇ       ‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ evaluation_*.txt
‚îÇ       ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ follow_up_question.txt
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ main.py         # FastAPI application
‚îÇ       ‚îú‚îÄ‚îÄ requirements.txt    # Python dependencies
‚îÇ       ‚îî‚îÄ‚îÄ .env                # Environment variables
‚îÇ
‚îú‚îÄ‚îÄ packages/                   # Shared packages
‚îÇ   ‚îú‚îÄ‚îÄ eslint-config/          # ESLint configuration
‚îÇ   ‚îú‚îÄ‚îÄ typescript-config/      # TypeScript configuration
‚îÇ   ‚îî‚îÄ‚îÄ ui/                     # Shared UI components
‚îÇ
‚îú‚îÄ‚îÄ docs/                       # Documentation
‚îÇ   ‚îú‚îÄ‚îÄ README.md
‚îÇ   ‚îú‚îÄ‚îÄ SETUP.md
‚îÇ   ‚îú‚îÄ‚îÄ DEBUGGING.md
‚îÇ   ‚îî‚îÄ‚îÄ ARCHITECTURE.md
‚îÇ
‚îú‚îÄ‚îÄ scripts/                    # Utility scripts
‚îú‚îÄ‚îÄ package.json               # Root dependencies
‚îú‚îÄ‚îÄ turbo.json                 # Turbo configuration
‚îî‚îÄ‚îÄ yarn.lock                  # Dependency lock file
```

## üîß Component Details

### Frontend Architecture

#### **React Components Hierarchy**
```
App
‚îú‚îÄ‚îÄ BrowserRouter
‚îÇ   ‚îú‚îÄ‚îÄ LandingPage          # Interview introduction
‚îÇ   ‚îú‚îÄ‚îÄ Instructions         # Pre-interview setup
‚îÇ   ‚îú‚îÄ‚îÄ InterviewWindow      # Main interview interface
‚îÇ   ‚îú‚îÄ‚îÄ Completion          # Results and feedback
‚îÇ   ‚îî‚îÄ‚îÄ NotFound            # 404 page
```

#### **State Management**
- **Local State**: React hooks (useState, useEffect)
- **API State**: TanStack Query for server state
- **Interview State**: Custom hooks for interview flow
- **Global State**: Context API for shared state

#### **Key Components**
- **InterviewWindow**: Main interview interface with question display and answer capture
- **Completion**: Results visualization with scores and feedback
- **UI Components**: shadcn/ui for consistent, accessible design
- **API Client**: Axios-based client with interceptors

### Backend Architecture

#### **FastAPI Application Structure**
```python
# main.py - Application entry point
app = FastAPI(
    title="Autonomous Interview API",
    docs_url="/docs",
    redoc_url="/redoc"
)

# CORS configuration for frontend access
app.add_middleware(CORSMiddleware, ...)

# Route registration
app.include_router(interview.router, prefix="/api/v1/interview")
```

#### **Database Models**
```python
# SQLAlchemy models with async support
class InterviewSession(Base):
    id: Mapped[UUID]
    title: Mapped[str]
    description: Mapped[str]
    created_at: Mapped[datetime]
    
class Question(Base):
    id: Mapped[UUID]
    text: Mapped[str]
    question_type: Mapped[str]
    difficulty: Mapped[str]
    
class InterviewResponse(Base):
    id: Mapped[UUID]
    session_id: Mapped[UUID]
    question_id: Mapped[UUID]
    answer: Mapped[JSON]
    evaluation: Mapped[JSON]
```

#### **Service Layer**
- **LLMService**: Handles AI model interactions and evaluation
- **ChromaService**: Manages vector storage and similarity search  
- **AuthService**: User authentication and authorization
- **CacheService**: Redis-based caching with fallbacks

### AI/ML Architecture

#### **LLM Integration**
```python
class LLMService:
    def __init__(self):
        self.client = Groq(api_key=settings.GROQ_API_KEY)
        self.model = "mixtral-8x7b-32768"
        
    async def evaluate_response(self, question, answer):
        # Template-based prompt generation
        prompt = self.prompts[f"evaluation_{question.type}"].format(
            question=question.text,
            answer=json.dumps(answer)
        )
        
        # LLM inference with structured output
        response = await self.client.chat.completions.create(
            messages=[...],
            model=self.model,
            response_format={"type": "json_object"}
        )
        
        return EvaluationResult(**json.loads(response.content))
```

#### **Vector Storage**
```python
class ChromaService:
    def __init__(self):
        self.client = chromadb.PersistentClient(path="./chroma_data")
        
    def get_or_create_collection(self, name):
        return self.client.get_or_create_collection(
            name=name,
            embedding_function=embedding_functions.DefaultEmbeddingFunction(),
            metadata={"hnsw:space": "cosine"}
        )
```

## üîÑ Data Flow

### Interview Session Flow
```mermaid
sequenceDiagram
    participant U as User
    participant F as Frontend
    participant A as API
    participant L as LLM Service
    participant C as ChromaDB
    participant D as Database
    
    U->>F: Start Interview
    F->>A: POST /sessions/create
    A->>D: Create session
    A->>C: Get similar questions
    A-->>F: Session + Questions
    
    loop For each question
        F->>U: Display question
        U->>F: Provide answer
        F->>A: POST /sessions/{id}/submit
        A->>L: Evaluate response
        L-->>A: Evaluation result
        A->>D: Store response + evaluation
        A-->>F: Feedback
    end
    
    F->>A: GET /sessions/{id}/complete
    A->>D: Get final results
    A-->>F: Complete results
    F->>U: Show completion screen
```

### AI Evaluation Pipeline
```mermaid
graph LR
    A[Question + Answer] --> B[Prompt Template]
    B --> C[LLM API Call]
    C --> D[Parse JSON Response]
    D --> E[Validation]
    E --> F[Store Results]
    
    subgraph "Fallback Path"
        G[Mock Evaluation]
        H[Default Scores]
    end
    
    C -.->|API Error| G
    G --> H
    H --> F
```

## üöÄ Deployment Architecture

### Development
- **Frontend**: Vite dev server (port 5000)
- **Backend**: Uvicorn with reload (port 8000)
- **Database**: SQLite with WAL mode
- **AI Services**: Mock responses (no API keys required)

### Production
- **Frontend**: Static build served by CDN
- **Backend**: Uvicorn with multiple workers
- **Database**: PostgreSQL with connection pooling
- **Cache**: Redis for session management
- **AI Services**: Real API keys with rate limiting
- **Monitoring**: Health checks and metrics

## üîê Security Considerations

### Current Implementation
- **CORS**: Configured for development origins
- **Environment Variables**: Secrets in .env files
- **Input Validation**: Pydantic schemas
- **SQL Injection**: SQLAlchemy ORM protection

### Production Recommendations
- **Authentication**: JWT tokens with refresh
- **Authorization**: Role-based access control
- **Rate Limiting**: API endpoint protection
- **HTTPS**: SSL/TLS encryption
- **Secrets Management**: External vault integration
- **Audit Logging**: User action tracking

## üìä Performance Considerations

### Current Optimizations
- **Async/Await**: Non-blocking I/O operations
- **Connection Pooling**: Database connection reuse
- **Lazy Loading**: Services initialized on demand
- **Caching**: Redis for frequently accessed data

### Scaling Strategies
- **Horizontal Scaling**: Multiple API server instances
- **Database Sharding**: Partition by user/session
- **CDN**: Static asset delivery
- **Load Balancing**: Request distribution
- **Caching Layers**: Multi-level caching strategy

This architecture provides a solid foundation for building an enterprise-grade AI interview platform while maintaining developer productivity and code quality.